# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1te_dVqWOGZaCo8VPYgYes9N3DAWkouYR
"""

#!/usr/bin/env python
# coding: utf-8

# # Project 1 NLA: Direct methods in optimization with constraints
#
# ### Ilaria Curzi

# ## Solving the KKT system
#

# ### T1: Show that the predictor steps reduces to solve a linear system with matrix MKKT.
#

# We want to optimize a quadratic objective function $f(x) = \frac{1}{2} x^T G x + g^T x,$ subject to equality $ A^T x = b$ and inequality constraints $C^T x \leq d. $
#
# To incorporate the constraints we use Lagrange multipliers. Inequality constraints are turned into equality constraints by introducing slack variables.
#
# The Lagrangian is:
#
# $$
# L(x, \lambda, \gamma, s) = \frac{1}{2} x^T G x + g^T x - \gamma^T (A^T x - b) - \lambda^T (C^T x - d - s),
# $$
#
# where $\lambda \in \mathbb{R^m}$$
# $ and  $\gamma  \in \mathbb{R^p}$ are the multipliers for the equality and inequality constraints.$G \in \mathbb{R}^{n \times n}$ is symmetric and positive semidefinite, $g \in \mathbb{R}^n, A \in \mathbb{R}^{n \times p},C \in \mathbb{R}^{n \times m},b \in \mathbb{R}^p$ ,e $d \in \mathbb{R}^m$
#
#
# The Karush-Kuhn-Tucker (KKT) conditions for this optimization are given by:
#
# 1. $$ Gx + g + A\gamma - C\lambda = 0 $$
#
# 2. $$ b - A^T x = 0 $$
#
# 3. $$ s + d - C^T x = 0 $$
#
# 4. $$ s_i\lambda_i = 0, \quad \text{i=1...m}  $$
#
# We also require the components $v_i \text{ of } v = (s, \lambda) \in \mathbb{R}^{2m}$  to satisfy  $v_i\geq0$ for all $i=1...m$ (feasibility region)
#
# To solve this nonlinear system of equations, we apply Newton's method.  We introduce $z=(x, \gamma, \lambda, s)$ and define the residual function $ F : \mathbb{R}^N \rightarrow \mathbb{R}^N, N = n+p+2m $. Each KKT equation is rewritten as a residual condition and the original nonlinear system is transformed into a linear system expressed as: $ M_{KKT} \cdot \Delta z = -F(z_0)$.
#
# In this case,
#
# $F(z_0) = \begin{pmatrix} r_L \\ r_A \\ r_C \\ r_s \end{pmatrix}$ consists in the residuals function derived from the KKT conditions,
#
# the vector $\Delta z = \begin{pmatrix} \Delta x \\ \Delta \gamma \\\Delta \lambda \\ \Delta s \end{pmatrix}$ represents the predictor direction for the variables (x,γ,λ,s) in each iteration.
#
#
# $M_{KKT} = \
# \begin{pmatrix}
# G & -A & -C & 0 \\
# -A^T & 0 & 0 & 0 \\
# -C^T & 0 & 0 & I \\
# 0 & 0 & S & \Lambda
# \end{pmatrix} $ is the KTT matrix
#
# Where $S$ is a diagonal matrix containing the values $s_i$ and $\Lambda$ is a diagonal matrix containing the values $\lambda_i$.
#
# Each iteration of the Newton method ultimately solves the linear system known as the KKT system.
# We start with a random point $z_0 = (x_0,γ_0,λ_0,s_0)$. After obtaining Δz we update the guess for z as: $z_{k+1} = z_k + 0.95 \alpha_k Δz$ , here $\alpha_k$ is a step reduction parameter that ensures that the new iteration
# $z_{k+1}$stays within the feasible region.
#
# To sum up, we are constructing a linear system from the KKT conditions and updating the variables with Newton's method. Each iteration of Newton's method aims to determine a predictor step that will guide the variables closer to satisfying the Karush-Kuhn-Tucker (KKT) conditions. It does this by focusing on finding a feasible direction that reduces the Karush-Kuhn-Tucker (KKT) residuals to zero.
#
# We conclude that the predictor steps reduces the problem to solving a linear system with the KKT matrix.

# ### C1: Write down a routine function that implements the step-size substep

# In[1]:


import time
import timeit
import random
import numpy as np
import sys
from matplotlib import pyplot as plt
from scipy.linalg import lu_factor, lu_solve
from scipy.linalg import ldl, solve_triangular, cholesky, lu_factor, lu_solve


# In[2]:


def Newton_step(lamb0, dlamb, s0, ds):

    alpha = 1
    index_lamb0 = np.array(np.where(dlamb < 0))

    if index_lamb0.size > 0:

        alpha = min(alpha,np.min(-lamb0[index_lamb0]/dlamb[index_lamb0]))
    index_s0 = np.array(np.where(ds<0))

    if index_s0.size > 0:

        alpha = min(alpha, np.min(-s0[index_s0]/ds[index_s0]))

    return alpha


# The **Newton_step** function limits the step $\alpha$ to ensure that the variables $\lambda$ and $s$ remain feasible (non-negative).
#
# Specifically, if $ d\lambda_i < 0 $ it means that the direction is reducing the value of $\lambda$. To prevent that $\lambda$ became negative, we choose $\alpha$ such that:
#
# $\lambda_{i+1}=\lambda_i + \alpha \cdot d\lambda \geq 0$
#
# which leads to the condition:
#
# $\alpha \leq -\frac{\lambda_{0,i}}{d\lambda_i}$
#
# where $\alpha$ is between 0 and 1.
#
# This represents the maximum permissible value of $\alpha$ for those indices to keep $\lambda$ non-negative.
#
# The function also checks the same condition for the slack variables $s$.
#

# # 2.1 Inequality constrains case (i.e. with A = 0)
# ### C2: Write down a program that, for a given n, implements the full algorithm for the test problem. Use the numpy.linalg.solve function to solve the KKT linear systems of the predictor and corrector substeps directly.       C3: Write a modification of the previous program C2 to report the computation time of the solution of the test problem for different dimensions n.

# In[3]:


def Matrix_KKT(G, C, n, m, lamb, s):
    #arg:
    #G : Coefficients for the quadratic objective function.
    #C : Constraint matrix.
    #n : Number of variables.
    #m  Number of constraints.
    #lamb : Lagrange multipliers.
    #s : Slack variables.

    S = np.diag(s)
    Lambdas = np.diag(lamb)
    eq1 = np.concatenate((G, -C, np.zeros((n, m))), axis = 1)
    eq2 = np.concatenate((np.transpose(-C), np.zeros((m, m)), np.identity(m)), axis = 1)
    eq3 = np.concatenate((np.zeros((m, n)), S, Lambdas), axis = 1)
    Mat = np.concatenate((eq1, eq2, eq3))

    #returns: The KKT matrix.
    #The diagonal matrix of slack variables (S).
    #The diagonal matrix of Lagrange multipliers (Lambdas)

    return Mat, S, Lambdas


# In[4]:


def F(x, G, g):
    #this function returns the value of the objective function.
    return 0.5 * np.transpose(x).dot(G).dot(x) + np.transpose(g).dot(x)


def F_z(x,lamb,s, G, g, C, d):

    eq1 = G.dot(x) + g - C.dot(lamb)
    eq2 = s + d - np.transpose(C).dot(x)
    eq3 = s * lamb
    #Returns a vector of equations representing KKT conditions.
    return np.concatenate((eq1, eq2, eq3))


# In[5]:


def Func_C3(n, maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "Yes"):
    np.random.seed(2) # Set the seed for random number generation
    random.seed(2)  # Set the seed for the random module

    # We define all the parameters we will need

    m = 2*n
    x = np.zeros((n))
    lamb = np.ones((m))
    s = np.ones((m))
    z = np.concatenate((x, lamb, s))
    G = np.identity(n)
    C = np.concatenate((G, - G), axis = 1)
    d = np.full((m), - 10)
    e = np.ones((m))
    g = np.random.normal(0, 1, (n))

    # We count the time if asked for when calling the function

    if Print_Time == "Yes":

        Start = time.time()

    # Create Matrix_KKT, S and Lambdas with the previously defined function

    Mat, S, Lambdas = Matrix_KKT(G, C, n, m, lamb,s)

    for i in range(maxIter):

        b = -F_z(x, lamb, s, G, g, C, d)
        delta = np.linalg.solve(Mat,b)

        # We use the step-size correction function previously defined

        alpha = Newton_step(lamb, delta[n:n+m], s, delta[n+m:])

        # We compute the correction param.

        mu = s.dot(lamb) / m
        Mu = ((s + alpha * delta[n+m:]).dot(lamb + alpha * delta[n:(n + m)])) / m
        sigma = (Mu/mu)**3

        # We correct the sub-step

        b[(n + m):] = b[(n + m):] - np.diag(delta[(n + m):]*delta[n:(n + m)]).dot(e) + sigma * mu * e

        # We find delta by using the linalg function

        delta = np.linalg.solve(Mat, b)

        #  Step-size correction with delta

        alpha = Newton_step(lamb, delta[n:(n + m)], s, delta[(n + m):])

        # Let's update the sub-step

        z = z + (alpha * delta) * 0.95

        # Stopping citeria

        if (np.linalg.norm(-b[:n]) < epsilon) or (np.linalg.norm(-b[n:(n + m)]) < epsilon) or (np.abs(mu) < epsilon):

            break

        # We update the Matrix_KKT

        x = z[:n]
        lamb = z[n:(n + m)]
        s = z[(n + m):]
        Mat, S, Lambdas = Matrix_KKT(G, C, n, m, lamb, s)

    # We print all the results obtained

    if Print_Time == "Yes":

        End = time.time()

        if Print_Results == "Yes":

            print("The computational time for the test problem is equal to: ", End - Start)

    if Print_Results == "Yes":

        print('The minimum of the function was found:', F(x, G, g))
        print('The real minimum is:', F(-g, G, g))
        print('Iterations needed:', i)
        print('Condition number:', np.linalg.cond(Mat))

    return(End - Start, i, abs(F(x, G, g) - F(-g, G, g)), np.linalg.cond(Mat))


# The algorithm follows an approach to ensure that the solution satisfies the feasibility condition.
#
# The idea is to modify the Newton steps so the the iterates $z_k$ stay close to a curve (the central path) of feasible points. The current balance of the system is represented by $\mu$ , which gives an indication of how far the system is from the central path, while Mu measures the progress after a correction step. The relationship between $\mu$ and Mu is fundamental to ensure the stability of the algorithm. If Mu increases significantly, it may indicate that the algorithm is making aggressive steps and is necessary to introduce a correction to guaratee that he system remains within the feasible region. Sigma is calculated specifically to bring the updates closer to the central path by gradually reducing $\mu$ without forcing it to zero immediatly but progressively, allowing the algorithm to remain stable and follow the central path.
#
# In this way, we navigate through the optimization process ensuring that the algorithm not only makes progress but does so in a controlled and stable manner.

# In[6]:


# We can call the function like this:
result = Func_C3(n=10, Print_Time="Yes", Print_Results="Yes")


# ## T2: Explain the previous derivations of the different strategies and justify under which assumptions they can be applied.

# We now propose two strategies for solving the KKT system, focusing on the inequality constraints (with A=0).
#
#   $
#    \begin{pmatrix}
#    G & -C & 0 \\
#    -C^T  & 0 & I \\
#    0 & S & \Lambda
#    \end{pmatrix}
#    \begin{pmatrix}
#    \delta_x \\
#    \delta_\lambda\\
#    \delta_s
#    \end{pmatrix}
#    =
#    \begin{pmatrix}
#    -r_1 \\
#    -r_2 \\
#    -r_3
#    \end{pmatrix}
#    $
#
# ### Strategy 1:
#
# 1. Isolate $\delta_s$ from the third row of the KKT conditions:
#
#    $S \delta_\lambda + \Lambda \delta_s = -r_3$
#
#    $\Lambda \delta_s = -r_3 -S\delta_\lambda$
#
#    $\delta_s = -\Lambda^{-1} r_3  -\Lambda^{-1}(S\delta_\lambda)$
#
#    $\delta_s = \Lambda^{-1} \cdot (-r_3 - S\delta_\lambda)$
#
#
# 2. Substitute $\delta_s$ into the second row of the KKT conditions.
#
#    $-C^T \delta_x + I \delta_s = -r_2$
#
#    $-C^T \delta_x + I \left( -\Lambda^{-1} r_3 - \Lambda^{-1} (S \delta_\lambda) \right) = -r_2$
#
#    $-C^T \delta_x - \Lambda^{-1} r_3 - \Lambda^{-1} S \delta_\lambda = -r_2$
#
#    $C^T \delta_x + \Lambda^{-1} S \delta_\lambda= r_2 - \Lambda^{-1} r_3$
#
#    This leads to a new smaller linear system:
#
#    $
#    \begin{pmatrix}
#    G & -C \\
#    -C^T & \Lambda^{-1} S
#    \end{pmatrix}
#    \begin{pmatrix}
#    \delta_x \\
#    \delta_\lambda
#    \end{pmatrix}
#    =-
#    \begin{pmatrix}
#    r_1 \\
#    r_2 - \Lambda^{-1} r_3
#    \end{pmatrix}
#    $
#
#
# 3. Use $LDL^T$ factorization on the resulting matrix, to efficiently solve the linear system.
#
# ### Strategy 2:
#
# 1. Isolate $\delta_s$ from the second row of the KKT conditions, so we have:
#
#    $-C^T\delta_x + I\delta_s = -r_2$
#
#    $\delta_s = -r_2 + C^T \delta_x$
#
# 2. Substitute this expression into the third row of the KKT conditions, so we get:
#
#    $S \delta_\lambda + \Lambda \delta_s = -r_3$
#
#    $\delta_\lambda S + \Lambda (-r_2 +C^T \delta_x) = -r_3$
#
#    $\delta_\lambda = S^{-1} (-r_3 + \Lambda r_2) - S^{-1} \Lambda C^T \delta_x$
#
# 3. Finally, we substitute into the first row to obtain a reduced linear system:
#
#    $G\delta_x - C \delta_\lambda = -r_1$
#
#    $G\delta_x - C(S^{-1} (-r_3 + \Lambda r_2) - S^{-1} \Lambda C^T \delta_x) = -r_1$
#
#    $G \delta_x + CS^{-1} r_3 - CS^{-1} \Lambda r_2 + CS^{-1} \Delta C^T \delta_x = -r_1$
#
#    $G\delta_x + CS^{-1} \Delta C^T \delta_x = -r_1 - CS^{-1} r_3 + CS^{-1} \Lambda r_2$
#
#    $\hat{G} \delta_x = -r_1 + CS^{-1}(-r_3 + \Lambda r_2)$
#
#    $\hat{G} \delta_x = -r_1 - \hat{r}$
#
#    where $\hat{G} = G + CS^{-1} \Delta C^T$ and $\hat{r} - CS^{-1} (r_3 + \Lambda r_2)$
#
# 4. Apply Cholesky factorization to $\hat{G}$ to solve the system.
#
#
# Both strategies simplify the KKT system, making it easier to solve with techniques like $LDL^T$ and Cholesky factorization.
#
# They rely on the invertibility of $S$ and $\Lambda$, as well as the consistency of the KKT conditions, to ensure a valid solution. Specifically, for the first strategy the matrix    $
#    \begin{pmatrix}
#    G & -C \\
#    -C^T & \Lambda^{-1} S
#    \end{pmatrix}
#    $ must be symmetric and positive semidefinite for $LDL^T$ factorization to work. For the second strategy,  $\hat{G}$ must be symmetric and positive semidefinite for Cholesky factorization to be applicable.
#
# When these assumptions are met, applying one of these strategies can significantly improve the efficiency and manageability of the solution process.
#

# ### C4. Write down two programs (modifcations of C2) that solve the optimization problem for the test problem using the previous strategies. Report the computational time for different values of n and compare with the results in C3.

# We first start with the first strategy

# In[7]:


# We create the KKT Matrix, lambda and S matrices.

def M_KKT_1(G, C, lamb, s):

    S = np.diag(s)
    Lambdas = np.diag(lamb)
    eq1 = np.concatenate((G, -C),axis = 1)
    eq2 = np.concatenate((- np.transpose(C), - np.diag(1 / lamb * s)), axis = 1)
    Mat = np.concatenate((eq1, eq2))

    return Mat, S, Lambdas


# In[8]:


def Func_C4_strategy_1(n, maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "No"):
    np.random.seed(2) # Set the seed for random number generation
    random.seed(2)  # Set the seed for the random module

    # We define all the parameters we will need

    m = 2*n
    x = np.zeros((n))
    lamb = np.ones((m))
    s = np.ones((m))
    z = np.concatenate((x,lamb,s))
    G = np.identity(n)
    C = np.concatenate((G,-G),axis = 1)
    d = np.full((m), -10)
    e = np.ones((m))
    g = np.random.normal(0, 1, (n))

    # We count the time if asked for when calling the function

    if Print_Time == "Yes":
      np.random.seed(4)
      Start = time.time()

    # Create Matrix_KKT, S and Lambdas with the previously defined function

    Mat, S, Lambda = M_KKT_1(G, C, lamb, s)

    for i in range(maxIter):

        lamb_inv = np.diag(1/lamb)

        b = F_z(x, lamb, s, G, g, C, d)
        r1 = b[:n]
        r2 = b[n:(n + m)]
        r3 = b[(n + m):]
        b = np.concatenate(([- r1, - r2 + 1/ lamb * r3]))

        # LDL factorization

        L, D, perm = ldl(Mat)
        y = solve_triangular(L, b, lower=True, unit_diagonal = True)
        delta = solve_triangular(D.dot(np.transpose(L)), y, lower = False)
        deltaS = lamb_inv.dot(- r3 - s * delta[n:])
        delta = np.concatenate((delta, deltaS))

        # We use th step-size correction function previously defined

        alpha = Newton_step(lamb, delta[n:(n + m)], s, delta[(n + m):])

        # We compute the correction param.

        mu = s.dot(lamb) / m
        Mu = ((s + alpha * delta[(n + m):]).dot(lamb + alpha * delta[n:(n + m)])) / m
        sigma = (Mu / mu) ** 3

        # Corrector substep

        Ds = np.diag(delta[(n + m):] * delta[n:(n + m)])
        b = np.concatenate((-r1, -r2 + lamb_inv.dot(r3 + Ds.dot(e) - sigma * mu * e)))

        # We repeat the LDL factorization

        y = solve_triangular(L, b, lower = True, unit_diagonal = True)
        delta = solve_triangular(D.dot(np.transpose(L)), y, lower = False)
        deltaS = lamb_inv.dot(-r3 - Ds.dot(e) + sigma * mu * e - s * delta[n:])
        delta = np.concatenate((delta, deltaS))

        # Step-size correction substep

        alpha = Newton_step(lamb, delta[n:(n + m)], s, delta[(n + m):])

        # Update substep

        z = z + (alpha * delta) * 0.95

        # Stopping citeria

        if (np.linalg.norm(- b[:n]) < epsilon) or (np.linalg.norm(- b[n:(n + m)]) < epsilon) or (np.abs(mu) < epsilon):
            break

        # We update the Matrix_KKT

        x = z[:n]
        lamb = z[n:(n + m)]
        s = z[(n + m):]
        Mat, S, Lambda = M_KKT_1(G, C, lamb, s)

    # We print all the results obtained

    if Print_Time == "Yes":

        End = time.time()

        if Print_Results == "Yes":

            print("The computational time for the test problem is equal to: ", End - Start)

    if Print_Results == "Yes":

        print('The minimum of the function was found:', F(x, G, g))
        print('The real minimum is:', F(-g, G, g))
        print('Iterations needed:', i)
        print('Condition number:', np.linalg.cond(Mat))

    return(End - Start, i, abs(F(x, G, g) - F(-g, G, g)), np.linalg.cond(Mat))


# In[9]:


#We just call the function once to check our values
#n=10
Func_C4_strategy_1(n=10, maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "Yes")


# We now implement the second strategy

# In[10]:


# We create the KKT Matrix, lambda and S matrices.

def M_KKT_2(G, C, lamb, s):

    S = np.diag(s)
    Lambdas = np.diag(lamb)
    Mat = G + C.dot(np.diag(1 / s * lamb)).dot(np.transpose(C))

    return Mat, Lambdas, S


# In[11]:


def Func_C4_strategy_2(n, maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "No"):
    np.random.seed(2) # Set the seed for random number generation
    random.seed(2)  # Set the seed for the random module

    # We define all the parameters we will need

    m = 2 * n
    x = np.zeros((n))
    lamb = np.ones((m))
    s = np.ones((m))
    z = np.concatenate((x, lamb, s))
    G = np.identity(n)
    C = np.concatenate((G, - G),axis = 1)
    d = np.full((m), - 10)
    e = np.ones((m))
    g = np.random.normal(0, 1, (n))

    # We count the time if asked for when calling the function

    if Print_Time == "Yes":
        np.random.seed(4)
        Start = time.time()

    # Create Matrix_KKT, S and Lambdas with the previously defined function

    Ghat, Lambda, S  = M_KKT_2(G, C, lamb,s)

    for i in range(maxIter):

        S_inv = np.diag(1 / s)

        b = F_z(x, lamb, s, G, g, C, d)
        r1 = b[:n]
        r2 = b[n:(n + m)]
        r3 = b[(n + m):]
        rhat = - C.dot(np.diag(1 / s)).dot((- r3 + lamb * r2))
        b = - r1 - rhat

        # Cholesky factorization

        Cholesk = cholesky(Ghat, lower = True)
        y = solve_triangular(Cholesk, b, lower=True)
        delta_x = solve_triangular(np.transpose(Cholesk), y)
        delta_lamb = S_inv.dot((- r3 + lamb * r2)) - S_inv.dot(Lambda.dot(np.transpose(C)).dot(delta_x))
        delta_s = - r2 + np.transpose(C).dot(delta_x)
        delta = np.concatenate((delta_x,delta_lamb, delta_s))

        # We use th step-size correction function previously defined

        alpha = Newton_step(lamb, delta[n:(n + m)], s, delta[(n + m):])

        # We compute the correction param.

        mu = s.dot(lamb) / m
        Mu = ((s + alpha * delta[(n + m):]).dot(lamb + alpha * delta[n:(n + m)])) / m
        sigma = (Mu / mu) ** 3

        # Corrector substep

        Ds_Dlamb = np.diag(delta[n+m:]*delta[n:n+m])
        b = -r1-(-C.dot(np.diag(1/s)).dot((-r3-Ds_Dlamb.dot(e)+sigma*mu*e+lamb*r2)))

        # We repeat the Cholesky factorization again

        y = solve_triangular(Cholesk,b,lower=True)
        delta_x = solve_triangular(np.transpose(Cholesk),y)
        delta_lamb = S_inv.dot(-r3-Ds_Dlamb.dot(e)+sigma*mu*e+lamb*r2)-S_inv.dot(lamb*(np.transpose(C).dot(delta_x)))
        delta_s = - r2 + np.transpose(C).dot(delta_x)
        delta = np.concatenate((delta_x,delta_lamb, delta_s))

        # Step-size correction substep

        alpha = Newton_step(lamb, delta[n:(n + m)],s,delta[(n + m):])

        # Update substep

        z = z + (alpha * delta) * 0.95

        # Stopping citeria

        if (np.linalg.norm(- r1) < epsilon) or (np.linalg.norm(-r2) < epsilon) or (np.abs(mu) < epsilon):

            break

        # We update the Matrix_KKT

        x = z[:n]
        lamb = z[n:(n + m)]
        s = z[(n + m):]
        Ghat, Lambda, S = M_KKT_2(G, C, lamb,s)

    if Print_Time == "Yes":

        End = time.time()

        if Print_Results == "Yes":

            print("Computation time for the test problem: ", End - Start)

    if Print_Results == "Yes":

        print('The minimum of the function was found:', F(x, G, g))
        print('The real minimum is:', F(-g, G, g))
        print('Iterations needed:', i)
        print('Condition number:', np.linalg.cond(Ghat))

    return(End - Start, i, abs(F(x, G, g) - F(-g, G, g)), np.linalg.cond(Ghat))


# In[12]:


#We just call the function once to check our values
#n=10
Func_C4_strategy_2(n = 10, Print_Time = "Yes", Print_Results = "Yes")


# We will compare the results of each function/program (C3, C4_LDL, C4_Cholesky) for different values of n (10, 50, 100):

# n=100

# In[13]:


print("C3")
Func_C3(n=100, Print_Time="Yes", Print_Results="Yes")
print("\n")

print("C4_LDL")
Func_C4_strategy_1(n=100, maxIter=100, epsilon=10e-16, Print_Time="Yes", Print_Results="Yes")
print("\n")

print("C4_Cholesky")
Func_C4_strategy_2(n=100, maxIter=100, epsilon=10e-16, Print_Time="Yes", Print_Results="Yes")


# n=500

# In[14]:


print("C3")
Func_C3(n=500, Print_Time="Yes", Print_Results="Yes")
print("\n")

print("C4_LDL")
Func_C4_strategy_1(n=500, maxIter=100, epsilon=10e-16, Print_Time="Yes", Print_Results="Yes")
print("\n")

print("C4_Cholesky")
Func_C4_strategy_2(n=500, maxIter=100, epsilon=10e-16, Print_Time="Yes", Print_Results="Yes")


# n=1000

# In[15]:


print("C3")
Func_C3(n=1000, Print_Time="Yes", Print_Results="Yes")
print("\n")

print("C4_LDL")
Func_C4_strategy_1(n=1000, maxIter=100, epsilon=10e-16, Print_Time="Yes", Print_Results="Yes")
print("\n")

print("C4_Cholesky")
Func_C4_strategy_2(n=1000, maxIter=100, epsilon=10e-16, Print_Time="Yes", Print_Results="Yes")


# We observe that for each n, all methods converge precisely to the minimum, demonstrating that they are efficent in reaching the optimal solution despite differences in their computational efficiency and stability.
#
# The Cholesky factorization consistely remain the fastest accross all sizes. This difference between methods is particularly notable for n = 1000.
#
# As reguard the condition number, Cholesky factorization mantein a perfect condition numeber of 1.0 accross all cases, indicating that is numericcaly stable and well conditioned. In contrast, the high condition number and instability of the $LDL^T$ factorization become evident for n=100 and n=1000.
#
# Also, for a given $n$ the difference in iterations needed between the strategies is minimal. This stability sugget that the problem complexity depends more on the method used than on the number of variables.
#
# In summary, Cholesky factorization is the most robust choice for all sizes, but as the problem size decrease, the performance gap between methods also decrease, especially in terms of iterations and computational time.

# ### C5: Write down a program that solves the optimization problem for the general case. Use numpy.linalg.solve function. Read the data of the optimization problems from the les (available at the Campus Virtual). Each problem consists on a collection of files: G.dad, g.dad,A.dad, b.dad, C.dad and d.dad. They contain the corresponding data in coordinate format. Take as initial condition $x0 = (0..0)$ and $s_0=\gamma_0= \lambda_0= (1...1)$ for all problems.

# In[16]:


def ReadMatrix(source, shape, symm=False):

    matrix = np.zeros(shape)

    with open(source, "r") as file:

        a = file.readlines()

    for line in a:

        row, column, value = line.strip().split()
        row = int(row)
        column = int(column)
        value = float(value)
        matrix[row - 1, column - 1] = value

        if symm == True:

            matrix[column - 1, row - 1] = value

    return matrix


def ReadVector(source, n):

    v = np.zeros(n)

    with open(source, "r") as file:

        a = file.readlines()

    for line in a:

        idx, value = line.strip().split()
        idx = int(idx)
        value = float(value)
        v[idx - 1] = value

    return v


# In[17]:


# We define the Matrix KKT for the exercise

def M_KKT_C5(G, C, A, n, m, p, lamb,s):

    S = np.diag(s)
    Lambda = np.diag(lamb)
    temp1 = np.concatenate((G, -A, -C, np.zeros((n, m))),axis = 1)
    temp2 = np.concatenate((- np.transpose(A),np.zeros((p, p + 2 * m))), axis = 1)
    temp3 = np.concatenate((np.transpose(- C),np.zeros((m, p + m)), np.identity(m)), axis = 1)
    temp4 = np.concatenate((np.zeros((m, n + p)), S, Lambda), axis = 1)
    M = np.concatenate((temp1, temp2, temp3, temp4))

    return M, S, Lambda



def funC5(A, G, C, g, x, gamma, lamb, s, bm, d):

    comp1 = G.dot(x)+g-A.dot(gamma)-C.dot(lamb)
    comp2 = bm-np.transpose(A).dot(x)
    comp3 = s+d-np.transpose(C).dot(x)
    comp4 = s*lamb

    return np.concatenate((comp1,comp2,comp3,comp4))


# In[18]:


def Function_C5(maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "No", Data = r"optpr1-20241108"):

    np.random.seed(2) # Set the seed for random number generation
    random.seed(2)  # Set the seed for the random module

    # We define all the parameters we will need

    n = int(np.loadtxt(Data + "/g.dad")[:,0][-1])
    p = n // 2
    m = 2 * n
    A = ReadMatrix(Data + "/A.dad", (n, p))
    bm = ReadVector(Data + "/b.dad", p)
    C = ReadMatrix(Data + "/C.dad", (n, m))
    d = ReadVector(Data + "/d.dad", m)
    e = np.ones((m))
    G = ReadMatrix(Data + "/g.dad", (n, n), True)
    g = np.zeros(n)
    x = np.zeros((n))
    gamma = np.ones((p))
    lamb = np.ones((m))
    s = np.ones((m))
    z = np.concatenate((x,gamma,lamb,s))

    if Print_Time == "Yes":
        np.random.seed(2)
        Start = time.time()

    # Create Matrix_KKT, S and Lambdas with the previously defined function

    Mat, S, Lambda = M_KKT_C5(G, C, A, n, m, p, lamb, s)

    for i in range(maxIter):

        b = - funC5(A, G, C, g, x, gamma, lamb, s, bm, d)
        delta = np.linalg.solve(Mat, b)

        # Step-size correction substep

        alpha = Newton_step(lamb,delta[(n + p) : (n + p + m)], s, delta[(n + m + p):])

        # Compute correction parameters

        mu = s.dot(lamb) / m
        Mu = ((s + alpha * delta[(n + m + p):]).dot(lamb + alpha * delta[(n + p):(n + m + p)])) / m
        sigma = (Mu / mu) ** 3

        # Corrector substep

        b[(n + m + p):] = b[(n + p + m):] - np.diag(delta[(n + p + m):] * delta[(n + p) : (n + p + m)]).dot(e) + sigma * mu * e
        delta = np.linalg.solve(Mat, b)

        # Step-size correction substep

        alpha = Newton_step(lamb, delta[(n + p):(n + p + m)], s, delta[(n + m + p):])

        # We update the substep

        z = z + 0.95 * alpha * delta

        # The stopping criteria

        if (np.linalg.norm(- b[:n]) < epsilon) or (np.linalg.norm(- b[n:(n + m)]) < epsilon) or (np.linalg.norm(- b[(n + p):(n + p + m)]) < epsilon) or (np.abs(mu) < epsilon):

            break

        # We update the Matrix KKT

        x = z[:n]
        gamma = z[n:(n+p)]
        lamb = z[(n + p):(n + m + p)]
        s = z[(n + m + p):]
        Mat, S, Lambda = M_KKT_C5(G, C, A, n, m, p, lamb,s)

    condition_number = np.linalg.cond(Mat)

    if Print_Time == "Yes":

        End = time.time()

        if Print_Results == "Yes":

            print("Computation time: ",End - Start)

    if Print_Results == "Yes":

        print('Minimum was found:', F(x, G, g))
        print('Condition number:', condition_number)
        print('Iterations needed:', i)


# In[19]:


print("For matrices and vectors from optpr1, the obtained results are the following:")
print("\n")
Function_C5(maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "Yes", Data = r"optpr1-20241108")

print("\n")

print("For matrices and vectors from optpr2, the obtained results are the following:")
print("\n")
Function_C5(maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "Yes", Data = r"optpr1-20241108")


# We observe that optpr1 is computationally more efficient and faster to solve. However, the fact that both methods require a similar number of iterations suggests that the significant time difference is not due to the number of iterations, but rather the complexity and scale of the problems, as indicated by the large condition numbers. The larger condition number in optpr2 indicates that this problem is more sensitive to perturbations in the data, which contributes to the higher computational time.

# ### T3. Isolate $\delta_s$ from the 4th row of MKKT and substitute into the 3rd row. Justify that this procedure leads to a linear system with a symmetric matrix.

# $M_{KKT} = \
# \begin{pmatrix}
# G & -A & -C & 0 \\
# -A^T & 0 & 0 & 0 \\
# -C^T & 0 & 0 & I \\
# 0 & 0 & S & \Lambda
# \end{pmatrix}
# \begin{pmatrix}
# \delta_x \\
# \delta_\gamma\\
# \delta_\lambda\\
# \delta_\lambda
# \end{pmatrix}
# =-
# \begin{pmatrix}
# r_1 \\
# r_2 \\
# r_3 \\
# r_4 \\
# \end{pmatrix}
# $
#
# 1. Isolate $\delta_s$ from the fourth row of the KKT conditions:
#
#    $S\delta_\lambda + \Lambda \delta_s = -r_4$
#
#    $\delta_s = -\Lambda^{-1}(r_4 + S\delta_\lambda)$
#
# 2. Substitute $\delta_s$ into the third row of the KKT conditions.
#
#    $-C^T \delta_x + \delta_s = -r_3$
#
#    $-C^T \delta_x -\Lambda^{-1} (r_4 + S \delta_\lambda) = -r_3$
#
#    This leads to a new smaller linear system:
#
#    $
# \begin{pmatrix}
# G & -A & -C \\
# -A^T & 0 & 0 \\
# -C^T & 0 & -S\Lambda^{-1}
# \end{pmatrix}
# \begin{pmatrix}
# \delta_x \\
# \delta_\lambda \\
# \delta_s
# \end{pmatrix}
# =
# \begin{pmatrix}
# - r_1 \\
# - r_2 \\
# \Lambda^{-1}r_4 - r_3
# \end{pmatrix}
# $
#
#
# Where, the matrix G, is symmetric and positive semidefinite by assumption. The matrix $-S\Lambda^{-1}$ is also symmetric because both S and $\Lambda$ are diagonal (symmetric) matrices. Moreover, the matrix A and $A^T$, C and $C^T$ are placed in symmetric positions across the main diagonal. This ensures that the entire matrix is symmetric, as each element in a matrix above the diagonal has an identical counterpart below the diagonal.

# ### C6: Implement a routine that uses LDLT to solve the optimizations problems (in C5) and compare the results

# In[20]:


# We define the Matrix KKT for the exercise

def M_KKT_C6(G, C, A, n, m, p, lamb,s):

    S = np.diag(s)
    Lambda = np.diag(lamb)
    temp1 = np.concatenate((G,- A, - C),axis = 1)
    temp2 = np.concatenate((- np.transpose(A), np.zeros((p, p + m))), axis = 1)
    temp3 = np.concatenate((- np.transpose(C), np.zeros((m, p)), np.diag(-1 / lamb * s)), axis = 1)
    Mat = np.concatenate((temp1, temp2, temp3))

    return Mat, S, Lambda


# In[21]:


def Function_C6(maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "No", Data = "optpr1-20241108"):
    np.random.seed(2) # Set the seed for random number generation
    random.seed(2)  # Set the seed for the random module

    # We define all the parameters we will need

    n = int(np.loadtxt(Data + "/g.dad")[:,0][-1])
    p = n // 2
    m = 2 * n
    A = ReadMatrix(Data + "/A.dad", (n, p))
    bm = ReadVector(Data + "/b.dad", p)
    C = ReadMatrix(Data + "/C.dad", (n, m))
    d = ReadVector(Data + "/d.dad", m)
    e = np.ones((m))
    G = ReadMatrix(Data + "/g.dad", (n, n), True)
    g = np.zeros((n))
    x = np.zeros((n))
    gamma = np.ones((p))
    lamb = np.ones((m))
    s = np.ones((m))
    z = np.concatenate((x, gamma, lamb, s))

    if Print_Time == "Yes":
        np.random.seed(2)
        Start = time.time()

    # Create Matrix_KKT, S and Lambdas with the previously defined function

    Mat,S,Lamb = M_KKT_C6(G, C, A, n, m, p, lamb,s)

    for i in range(maxIter):

        lamb_inv = np.diag(1/lamb)

        b = funC5(A, G, C, g, x, gamma, lamb, s, bm, d)
        r1, r2, r3, r4 = b[:n], b[n:n+p], b[n+p:n+p+m], b[n+p+m:]
        b = np.concatenate(([-r1,-r2,-r3+1/lamb*r4]))

        # LDL factorization

        L, D, perm = ldl(Mat)
        y = np.linalg.solve(L, b)
        delta = np.linalg.solve(D.dot(np.transpose(L)), y)
        deltaS = lamb_inv.dot(- r4 - s * delta[(n + p):])
        delta = np.concatenate((delta, deltaS))

        # Step-size correction substep

        alpha = Newton_step(lamb,delta[(n + p) : (n + p + m)], s, delta[(n + m + p):])

        # We compute the correction parameters

        mu = s.dot(lamb) / m
        Mu = ((s + alpha * delta[(n + m + p):]).dot(lamb + alpha * delta[(n + p):(n + m + p)])) / m
        sigma = (Mu / mu) ** 3

        # Substep corrector

        Ds = np.diag(delta[(n + p + m):] * delta[(n + p):(n + p + m)])
        b = np.concatenate((- r1, - r2, - r3 + lamb_inv.dot(r4 + Ds.dot(e) - sigma * mu * e)))

        # Repeat LDL factorization

        y = np.linalg.solve(L, b)
        delta = np.linalg.solve(D.dot(np.transpose(L)), y)
        deltaS = lamb_inv.dot(- r4 - Ds.dot(e) + sigma * mu * e - s * delta[(n + p):])
        delta = np.concatenate((delta, deltaS))

        # Step-size correction substep

        alpha = Newton_step(lamb, delta[(n + p):(n + p + m)], s, delta[(n + m + p):])

        # We update the substep

        z = z + 0.95 * alpha * delta

        # The stopping criteria

        if (np.linalg.norm(- b[:n]) < epsilon) or (np.linalg.norm(- b[n:(n + m)]) < epsilon) or (np.linalg.norm(- b[(n + p):(n + p + m)]) < epsilon) or (np.abs(mu) < epsilon):

            break

        # We update tha  Matrix KKT

        x = z[:n]
        gamma = z[n:(n+p)]
        lamb = z[(n + p):(n + m + p)]
        s = z[(n + m + p):]
        Mat,Lamb,S = M_KKT_C6(G, C, A, n, m, p, lamb,s)

    condition_number = np.linalg.cond(Mat)

    if Print_Time == "Yes":

        End = time.time()

        if Print_Results == "Yes":

            print("Computation time: ",End - Start)

    if Print_Results == "Yes":

        print('Minimum was found:', F(x, G, g))
        print('Condition number:', condition_number)
        print('Iterations needed:', i)


# In[1]:


print("For matrices and vectors from optpr1, the obtained results where the following:")
print("\n")
Function_C6(maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "Yes", Data = r"optpr1-20241108")

print("\n")

print("For matrices and vectors from optpr2, the obtained results where the following:")
print("\n")
Function_C6(maxIter=100, epsilon=10e-16, Print_Time = "Yes", Print_Results = "Yes", Data = r"optpr2-20241108")


# We now compare the results obtained in C5 using the numpy.linalg.solve factorization function with those obtained in C6 using the LDLT factorization function.
#
# Both methods are able to find the same minimum, but the numpy.linalg.solve factorization is more numerically stable, as the condition number is significantly higher when using LDL factorization compared to numpy.linalg.solve factorization for both problems, particularly for optpr2. A higher condition number indicates that the problem is more sensitive to numerical errors or perturbations, which can lead to instability. This could explain why LDL factorization require longer computetion time and more iterations to converge. This could be due to the additional LDLT factorization and the step-size correction substeps involved in C6.