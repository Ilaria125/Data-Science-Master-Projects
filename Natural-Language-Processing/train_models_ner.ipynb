{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06ed32b-2501-4cff-a07c-b2b8de1d0e90",
   "metadata": {},
   "source": [
    "# train_models.ipynb\n",
    "In this notebook we train perceptron and 2 deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8aba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch import nn  # kept for completeness; can be removed if only using one\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Hugging Face Transformers & datasets\n",
    "from datasets import Dataset, Features, Sequence, Value, ClassLabel\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "# skseq (as given to us in the course)\n",
    "import skseq\n",
    "from skseq.sequences import sequence\n",
    "from skseq.sequences.sequence import Sequence\n",
    "from skseq.sequences.sequence_list import SequenceList\n",
    "from skseq.sequences.label_dictionary import LabelDictionary\n",
    "from skseq.sequences.extended_feature import ExtendedFeatures\n",
    "from skseq.sequences.id_feature import IDFeatures\n",
    "from skseq.readers.pos_corpus import PostagCorpus\n",
    "from skseq.sequences import structured_perceptron as spc\n",
    "\n",
    "\n",
    "from utils.utils import BiLSTM_NER, group_sentences, build_vocab, load_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3ee05-21ff-4200-931f-1b8248fdcba3",
   "metadata": {},
   "source": [
    "## 1. Perceptron training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8bd26f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helicopter/O gunships/O Saturday/B-tim pounded/O militant/O hideouts/O in/O the/O Orakzai/B-geo tribal/O region/O ,/O where/O many/O Taliban/B-org militants/O are/O believed/O to/O have/O fled/O to/O avoid/O an/O earlier/O military/O offensive/O in/O nearby/O South/B-geo Waziristan/I-geo ./O \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "train = pd.read_csv(\"data/train_data_ner.csv\", keep_default_na=False)\n",
    "\n",
    "# Initialise label dictionaries\n",
    "word_dict = LabelDictionary()\n",
    "tag_dict = LabelDictionary()\n",
    "\n",
    "for word in train['words'].unique():\n",
    "    if word not in word_dict:\n",
    "        word_dict.add(word)\n",
    "        \n",
    "for tag in train['tags'].unique():\n",
    "    if tag not in tag_dict:\n",
    "        tag_dict.add(tag)\n",
    "        \n",
    "# Initialize SequenceList\n",
    "train_seq = SequenceList(word_dict, tag_dict)\n",
    "\n",
    "# Group by sentence_id\n",
    "for _, group in train.groupby(\"sentence_id\"):\n",
    "    words = list(group[\"words\"])\n",
    "    tags = list(group[\"tags\"])\n",
    "    train_seq.add_sequence(words, tags, word_dict,tag_dict )\n",
    "\n",
    "# printing an example\n",
    "print(train_seq[1].to_words(train_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc200e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands/O of/O demonstrators/O have/O marched/O through/O London/B-geo to/O protest/O the/O war/O in/O Iraq/B-geo and/O demand/O the/O withdrawal/O of/O British/B-gpe troops/O from/O that/O country/O ./O \n"
     ]
    }
   ],
   "source": [
    "print(train_seq[0].to_words(train_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061567e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38366"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c78dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# feature_mapper = IDFeatures(train_seq)\n",
    "# feature_mapper.build_features()\n",
    "# sp1 = spc.StructuredPerceptron(word_dict, tag_dict, feature_mapper)\n",
    "# num_epochs = 1\n",
    "# sp.fit(feature_mapper.dataset, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d7dc5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.932807\n",
      "Epoch: 1 Accuracy: 0.945732\n",
      "Epoch: 2 Accuracy: 0.948930\n",
      "Epoch: 3 Accuracy: 0.951333\n",
      "Epoch: 4 Accuracy: 0.952946\n",
      "CPU times: user 10min 36s, sys: 18.8 s, total: 10min 55s\n",
      "Wall time: 10min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extended_features = ExtendedFeatures(train_seq)\n",
    "extended_features.build_features()\n",
    "sp = spc.StructuredPerceptron(word_dict, tag_dict, extended_features)\n",
    "num_epochs = 5 #2\n",
    "sp.fit(extended_features.dataset, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513852e5-7a16-4fac-9ee6-765780956cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.save_model(\"fitted_models/model1/parameters6\")\n",
    "print(\"Trained & saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d1e77f-6401-46d4-a4da-8d2c8d4c1813",
   "metadata": {},
   "source": [
    "## 2. Deep learning model 1 (BiLSTM) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b063170-3a42-4e20-9a35-5640f24fdd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 135.7674\n",
      "Epoch 2 Loss: 43.7626\n",
      "Epoch 3 Loss: 31.2912\n",
      "Epoch 4 Loss: 24.8312\n",
      "Epoch 5 Loss: 20.5942\n",
      "BiLSTM model saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"data/train_data_ner.csv\")\n",
    "train_sentences = group_sentences(train_df)  # returns list of (words, tags) tuples\n",
    "\n",
    "# Create vocabularies\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "tag2idx = {}\n",
    "idx2tag = {}\n",
    "\n",
    "for words, tags in train_sentences:  # Correct unpacking here\n",
    "    for word, tag in zip(words, tags):\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "        if tag not in tag2idx:\n",
    "            tag2idx[tag] = len(tag2idx)\n",
    "\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "\n",
    "# Encode sentences\n",
    "max_len = max(len(words) for words, tags in train_sentences)\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for words, tags in train_sentences:\n",
    "    x_seq = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in words]\n",
    "    y_seq = [tag2idx[tag] for tag in tags]\n",
    "\n",
    "    # Padding\n",
    "    x_seq += [word2idx[\"<PAD>\"]] * (max_len - len(x_seq))\n",
    "    y_seq += [0] * (max_len - len(y_seq))  # 0 for <PAD> label\n",
    "\n",
    "    X.append(x_seq)\n",
    "    y.append(y_seq)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTM_NER(vocab_size=len(word2idx), tagset_size=len(tag2idx)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        batch_y = batch_y.view(-1)\n",
    "\n",
    "        loss = loss_fn(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save model and mappings\n",
    "os.makedirs(\"fitted_models/model2\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"fitted_models/model2/lstm_model.pt\")\n",
    "\n",
    "with open(\"fitted_models/model2/idx_mappings.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"word2idx\": word2idx,\n",
    "        \"tag2idx\": tag2idx,\n",
    "        \"idx2tag\": idx2tag\n",
    "    }, f)\n",
    "\n",
    "print(\"BiLSTM model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7e618-c6e7-4324-b13a-ae88a412552d",
   "metadata": {},
   "source": [
    "## 3. Deep learning model 2 (BERT) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74049157-41c2-42db-a1ce-b13d724feea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21064bf6ed64e039bc42fa72bdf66f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c03141a4f24ae3a8d99238f8889ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14388' max='14388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14388/14388 2:33:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.402591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.516475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.536799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fitted_models/model3/tokenizer_config.json',\n",
       " 'fitted_models/model3/special_tokens_map.json',\n",
       " 'fitted_models/model3/vocab.txt',\n",
       " 'fitted_models/model3/added_tokens.json',\n",
       " 'fitted_models/model3/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import Sequence, Value, Features\n",
    "\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Load and preprocess data\n",
    "train_df, test_df, _ = load_data('data/train_data_ner.csv', 'data/test_data_ner.csv', 'data/tiny_test.csv')\n",
    "train_data = group_sentences(train_df)\n",
    "test_data = group_sentences(test_df)\n",
    "\n",
    "#  Build vocabulary for tags only \n",
    "_, tag2idx = build_vocab(train_data)\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "def to_hf_format(data, tag2idx):\n",
    "    hf_data = []\n",
    "    for words, tags in data:\n",
    "        tokens = list(words)  # list of str\n",
    "        label_ids = [tag2idx[tag] for tag in tags]  # list of int\n",
    "        hf_data.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"labels\": label_ids\n",
    "        })\n",
    "    return hf_data\n",
    "\n",
    "hf_train_data = to_hf_format(train_data, tag2idx)\n",
    "\n",
    "\n",
    "\n",
    "features = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(Value(\"int64\"))\n",
    "})\n",
    "\n",
    "train_dataset = Dataset.from_list(to_hf_format(train_data, tag2idx), features=features)\n",
    "test_dataset = Dataset.from_list(to_hf_format(test_data, tag2idx), features=features)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer and align labels\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding='max_length', max_length=100)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Define model and training arguments\n",
    "#model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2idx))\n",
    "\n",
    "#the below is to use apple silicon metal and speed up the process \n",
    "\n",
    "# Select MPS if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available, using CPU\")\n",
    "\n",
    "# Load model and move it to the selected device\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2idx)).to(device)\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_ner_output\",\n",
    "    evaluation_strategy=\"epoch\",       # <== must match save_strategy\n",
    "    save_strategy=\"epoch\",             # <== must match evaluation_strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "# Collator to pad inputs and labels correctly during training\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "# Collator to pad inputs and labels correctly during training\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "#  Train and save\n",
    "trainer.train()\n",
    "model.save_pretrained(\"fitted_models/model3\")\n",
    "tokenizer.save_pretrained(\"fitted_models/model3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06eb15-e5d7-4b1b-9033-02bce9660f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
