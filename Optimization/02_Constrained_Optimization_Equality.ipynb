{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR7hRBGJAKrX"
   },
   "source": [
    "# LAB 4\n",
    "\n",
    "# Georgia Zavou\n",
    "# Ilaria Curzi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7793fwQStIDs"
   },
   "source": [
    "# 1. One simple way to proceed is to take  and iteratively update the current point to obtain the next. This is a simple way to proceed that is proposed to perform first. The stopping condition should be performed over . Test this approach and check if it works using the starting point proposed in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFSTp0gCtONw"
   },
   "source": [
    "Initially we define the function that returns f and the Hessian. Then functions that compute the gradient and hessians given the point x1 and x2. Following we define functions to compute the Lagrange values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMPudYvhj12v"
   },
   "outputs": [],
   "source": [
    "from math import exp as e\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function f\n",
    "def f(x_1,x_2):\n",
    "  return e(3*x_1) + e(-4*x_2)\n",
    "\n",
    "# Hessian\n",
    "def h(x_1,x_2):\n",
    "  return x_1**2 + x_2**2 - 1\n",
    "\n",
    "# Gradient of  f\n",
    "def grad_f(x_1, x_2):\n",
    "    return np.array([3*e(3*x_1), -4*e(-4*x_2)])\n",
    "\n",
    "# Gradient of  constrained function\n",
    "def grad_h(x_1, x_2):\n",
    "    return np.array([2*x_1, 2*x_2])\n",
    "\n",
    "# Hessian of f\n",
    "def hess_f(x_1, x_2):\n",
    "    return np.array([[9*e(3*x_1), 0], [0, 16*e(-4*x_2)]])\n",
    "\n",
    "# Hessian of  constrained function\n",
    "def hess_h(x_1, x_2):\n",
    "    return np.array([[2, 0], [0, 2]])\n",
    "\n",
    "# Lagrange of f\n",
    "def lag(x_1, x_2, lamda):\n",
    "    return f(x_1, x_2) - lamda*h(x_1, x_2)\n",
    "\n",
    "# Lagrange of gradient\n",
    "def grad_lag(x_1, x_2, lamda):\n",
    "    return grad_f(x_1, x_2) - lamda*grad_h(x_1, x_2)\n",
    "\n",
    "# Lagrange of Hessian\n",
    "def hess_lag(x_1, x_2, lamda):\n",
    "    return hess_f(x_1, x_2) - lamda*hess_h(x_1, x_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-IS5cliuDs0"
   },
   "source": [
    "Now we define a function using Newtons step to iterate until we find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XzuQjRKtSOM"
   },
   "outputs": [],
   "source": [
    "def solveNewtonBased(x_1, x_2, lamda, alpha=1, eps=1e-5, MAX_ITER=100):\n",
    "\n",
    "    print(f'Starting point x0 = [{x_1}, {x_2}]')\n",
    "\n",
    "    for i in range(MAX_ITER):\n",
    "        # Compute the gradient of the constraint function h(x_1, x_2)\n",
    "        gh = grad_h(x_1, x_2)\n",
    "\n",
    "        # Compute the gradient of the Lagrangian\n",
    "        grad_lag_value = grad_lag(x_1, x_2, lamda)\n",
    "\n",
    "        # Compute the Hessian of the Lagrangian\n",
    "        hess_lag_value = hess_lag(x_1, x_2, lamda)\n",
    "\n",
    "        # Build the augmented system matrix A and the right-hand side vector b for the Newton step\n",
    "        # A is a block matrix combining the Hessian of the Lagrangian and the gradient of the constraint.\n",
    "        A = np.block([\n",
    "            [hess_lag_value, -gh.reshape(-1, 1)],  # First block: Hessian and negative constraint gradient\n",
    "            [-gh, np.array([[0]])]  # Second block: Negative constraint gradient and a scalar zero\n",
    "        ])\n",
    "\n",
    "        # b is the vector representing the right-hand side of the system, combining the negative gradient of the Lagrangian and the constraint value h(x_1, x_2).\n",
    "        b = np.concatenate([-grad_lag_value, [h(x_1, x_2)]])\n",
    "\n",
    "        # Solve the linear system A * delta = b to find the Newton step (delta)\n",
    "        delta = np.linalg.solve(A, b)\n",
    "\n",
    "        # Update the variables (x_1, x_2, lambda) by moving along the Newton direction\n",
    "        x_1 += alpha * delta[0]  # Update x_1\n",
    "        x_2 += alpha * delta[1]  # Update x_2\n",
    "        lamda += alpha * delta[2]  # Update lambda (Lagrange multiplier)\n",
    "\n",
    "        # Check if the gradient of the Lagrangian is small enough to satisfy the convergence criterion\n",
    "        if np.linalg.norm(grad_lag(x_1, x_2, lamda)) < eps:\n",
    "            # If the norm of the gradient is less than the tolerance, stop the iterations\n",
    "            break\n",
    "\n",
    "        print('\\nIterations: {}'.format(i))\n",
    "        print('\\nx = (x_1, x_2) = ({0:.5f}, {1:.5f}), lamda = {2:.5f}'.format(x_1, x_2, lamda))\n",
    "\n",
    "    # Return the final values of x_1, x_2, and lambda\n",
    "    return x_1, x_2, lamda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wgb__prLuvUX",
    "outputId": "dfeb89c7-d076-4241-e1b4-64ab8ca0a392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point x0 = [-1, 1]\n",
      "\n",
      "Iterations: 0\n",
      "\n",
      "x = (x_1, x_2) = (-0.77423, 0.72577), lamda = -0.35104\n",
      "\n",
      "Iterations: 1\n",
      "\n",
      "x = (x_1, x_2) = (-0.74865, 0.66614), lamda = -0.21606\n",
      "\n",
      "Obtained Result using Newton: (-0.7483381762503777, 0.663323446868971, -0.21232390186241443)\n"
     ]
    }
   ],
   "source": [
    "x_1, x_2, lamda = -1, 1, -1\n",
    "result = solveNewtonBased(x_1, x_2, lamda, alpha=1, eps=1e-5, MAX_ITER=100)\n",
    "\n",
    "print(\"\\nObtained Result using Newton:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrMwVqdhw2d4"
   },
   "source": [
    "We can see that the results are the same with the ones in the Lab4 PDF and we obtain them with 2 iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1VyrKaGu8LQ"
   },
   "source": [
    "# 2. This basic iteration also has drawbacks, leading to a number of vital questions. It is a Newtonlike iteration, and thus may diverge from poor starting points. In our example we have started from a point that is near to the optimal solution. Try to perform some experiments with starting points that are farther away of the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dUf9gIdBu-Yt",
    "outputId": "613e4f08-dd55-4823-e2b9-08c1466374be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.26550745 -7.91707189 -7.98360989]\n",
      " [-1.4286586  -1.8692028  -1.91740064]\n",
      " [ 2.17548553  2.41340769  6.42645673]]\n"
     ]
    }
   ],
   "source": [
    "# Larger ranges for starting points\n",
    "farther_ranges = [ (-8, -4), (-4, -1), (2, 8),]\n",
    "\n",
    "far_away_points = np.array([\n",
    "    np.random.uniform(low, high, 3) for low, high in farther_ranges\n",
    "])\n",
    "\n",
    "print(far_away_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mb1b4nvlzAEY",
    "outputId": "90433752-f4fd-4f68-8db2-541d1b594e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point:\n",
      "\n",
      "x = (x_1, x_2) = (-5.26551, -7.91707), lamda = -7.98361\n",
      "\n",
      "Starting point x0 = [-5.265507448361122, -7.91707189307672]\n",
      "\n",
      "Iterations: 0\n",
      "\n",
      "x = (x_1, x_2) = (2.84834, -7.66707), lamda = -12.30229\n",
      "\n",
      "Iterations: 1\n",
      "\n",
      "x = (x_1, x_2) = (-8.04633, -7.41707), lamda = -85829.04245\n",
      "\n",
      "Iterations: 2\n",
      "\n",
      "x = (x_1, x_2) = (-0.89725, -7.16707), lamda = -76258.25987\n",
      "\n",
      "Iterations: 3\n",
      "\n",
      "x = (x_1, x_2) = (25.62193, -6.91707), lamda = -2253914.70324\n",
      "\n",
      "Iterations: 4\n",
      "\n",
      "x = (x_1, x_2) = (25.28860, 42.68832), lamda = -59249653932822.30469\n",
      "\n",
      "Iterations: 5\n",
      "\n",
      "x = (x_1, x_2) = (24.95527, 14.06285), lamda = -39730996267044.75000\n",
      "\n",
      "Iterations: 6\n",
      "\n",
      "x = (x_1, x_2) = (24.62193, -14.48373), lamda = -80651091480492.07812\n",
      "\n",
      "Iterations: 7\n",
      "\n",
      "x = (x_1, x_2) = (24.28859, 13.08524), lamda = -218518191340249725286744064.00000\n",
      "\n",
      "Iterations: 8\n",
      "\n",
      "x = (x_1, x_2) = (23.95520, -15.34236), lamda = -474729432429685031563165696.00000\n",
      "\n",
      "Iterations: 9\n",
      "\n",
      "x = (x_1, x_2) = (23.61964, 10.47384), lamda = -6786748268420469466503053312.00000\n",
      "\n",
      "Iterations: 10\n",
      "\n",
      "x = (x_1, x_2) = (23.26863, -20.55619), lamda = -20106566518432475144664383488.00000\n",
      "\n",
      "Iterations: 11\n",
      "\n",
      "x = (x_1, x_2) = (2.79673, -20.30615), lamda = -8085951362781149069387329699840.00000\n",
      "\n",
      "Iterations: 12\n",
      "\n",
      "x = (x_1, x_2) = (-70.30527, -20.05330), lamda = -211353931591308907426733757562880.00000\n",
      "\n",
      "Iterations: 13\n",
      "\n",
      "x = (x_1, x_2) = (-32.37230, -19.79923), lamda = -114035308807872282839209134260224.00000\n",
      "\n",
      "Iterations: 14\n",
      "\n",
      "x = (x_1, x_2) = (-10.30444, -19.54163), lamda = -77736679319682219536176895754240.00000\n",
      "\n",
      "Iterations: 15\n",
      "\n",
      "x = (x_1, x_2) = (12.76439, -19.24396), lamda = -174031207829152075112956085403648.00000\n",
      "\n",
      "Iterations: 16\n",
      "\n",
      "x = (x_1, x_2) = (-7.34481, -18.75299), lamda = -274171172385541320198349490487296.00000\n",
      "\n",
      "Iterations: 17\n",
      "\n",
      "x = (x_1, x_2) = (10.13687, -14.81170), lamda = -652566108542789101071351817437184.00000\n",
      "\n",
      "Iterations: 18\n",
      "\n",
      "x = (x_1, x_2) = (5.08417, -7.42884), lamda = -325270342851872830337118491377664.00000\n",
      "\n",
      "Iterations: 19\n",
      "\n",
      "x = (x_1, x_2) = (2.57345, -3.76026), lamda = -160628233038743150348446061297664.00000\n",
      "\n",
      "Iterations: 20\n",
      "\n",
      "x = (x_1, x_2) = (1.34870, -1.97068), lamda = -76445830805163944144643697934336.00000\n",
      "\n",
      "Iterations: 21\n",
      "\n",
      "x = (x_1, x_2) = (0.79260, -1.15813), lamda = -31520187315695056909768465580032.00000\n",
      "\n",
      "Iterations: 22\n",
      "\n",
      "x = (x_1, x_2) = (0.59752, -0.87308), lamda = -7757964012252238301058106916864.00000\n",
      "\n",
      "Iterations: 23\n",
      "\n",
      "x = (x_1, x_2) = (0.56568, -0.82655), lamda = -413465369839340198916880596992.00000\n",
      "\n",
      "Iterations: 24\n",
      "\n",
      "x = (x_1, x_2) = (0.56478, -0.82524), lamda = -655183748795763439563702272.00000\n",
      "\n",
      "Iterations: 25\n",
      "\n",
      "x = (x_1, x_2) = (0.56478, -0.82524), lamda = -825198717526316417024.00000\n",
      "\n",
      "Iterations: 26\n",
      "\n",
      "x = (x_1, x_2) = (0.56478, -0.82524), lamda = -654442496.00000\n",
      "\n",
      "Iterations: 27\n",
      "\n",
      "x = (x_1, x_2) = (0.56478, -0.82524), lamda = 49.40335\n",
      "\n",
      "Iterations: 28\n",
      "\n",
      "x = (x_1, x_2) = (1.10508, -0.45547), lamda = -9.37357\n",
      "\n",
      "Iterations: 29\n",
      "\n",
      "x = (x_1, x_2) = (0.94148, -0.38184), lamda = 17.63973\n",
      "\n",
      "Iterations: 30\n",
      "\n",
      "x = (x_1, x_2) = (0.91542, -0.40395), lamda = 25.23760\n",
      "\n",
      "Iterations: 31\n",
      "\n",
      "x = (x_1, x_2) = (0.91034, -0.41400), lamda = 25.28732\n",
      "\n",
      "Iterations: 32\n",
      "\n",
      "x = (x_1, x_2) = (0.91041, -0.41370), lamda = 25.29385\n",
      "Starting point:\n",
      "\n",
      "x = (x_1, x_2) = (-1.42866, -1.86920), lamda = -1.91740\n",
      "\n",
      "Starting point x0 = [-1.4286586024559496, -1.8692027958204518]\n",
      "\n",
      "Iterations: 0\n",
      "\n",
      "x = (x_1, x_2) = (-0.16886, -1.61900), lamda = -1.75982\n",
      "\n",
      "Iterations: 1\n",
      "\n",
      "x = (x_1, x_2) = (2.12253, -1.34852), lamda = -66.02633\n",
      "\n",
      "Iterations: 2\n",
      "\n",
      "x = (x_1, x_2) = (1.31064, -0.65253), lamda = -616.41033\n",
      "\n",
      "Iterations: 3\n",
      "\n",
      "x = (x_1, x_2) = (0.96642, -0.46765), lamda = -163.79377\n",
      "\n",
      "Iterations: 4\n",
      "\n",
      "x = (x_1, x_2) = (0.90371, -0.43402), lamda = 12.25495\n",
      "\n",
      "Iterations: 5\n",
      "\n",
      "x = (x_1, x_2) = (0.90703, -0.42128), lamda = 25.17778\n",
      "\n",
      "Iterations: 6\n",
      "\n",
      "x = (x_1, x_2) = (0.91035, -0.41393), lamda = 25.28979\n",
      "\n",
      "Iterations: 7\n",
      "\n",
      "x = (x_1, x_2) = (0.91041, -0.41370), lamda = 25.29385\n",
      "Starting point:\n",
      "\n",
      "x = (x_1, x_2) = (2.17549, 2.41341), lamda = 6.42646\n",
      "\n",
      "Starting point x0 = [2.175485527053591, 2.4134076946107577]\n",
      "\n",
      "Iterations: 0\n",
      "\n",
      "x = (x_1, x_2) = (1.84463, 0.73161), lamda = 4.47791\n",
      "\n",
      "Iterations: 1\n",
      "\n",
      "x = (x_1, x_2) = (1.52046, -0.45888), lamda = 6.44246\n",
      "\n",
      "Iterations: 2\n",
      "\n",
      "x = (x_1, x_2) = (1.14191, -0.05439), lamda = -11.20427\n",
      "\n",
      "Iterations: 3\n",
      "\n",
      "x = (x_1, x_2) = (1.01029, 0.00363), lamda = 23.14762\n",
      "\n",
      "Iterations: 4\n",
      "\n",
      "x = (x_1, x_2) = (1.00054, -0.13267), lamda = 30.07991\n",
      "\n",
      "Iterations: 5\n",
      "\n",
      "x = (x_1, x_2) = (0.99573, -0.09852), lamda = 29.87039\n",
      "\n",
      "Iterations: 6\n",
      "\n",
      "x = (x_1, x_2) = (0.99498, -0.10004), lamda = 29.82795\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for point in far_away_points:\n",
    "    x_1, x_2, lamda = point\n",
    "    print('Starting point:\\n\\nx = (x_1, x_2) = ({0:.5f}, {1:.5f}), lamda = {2:.5f}\\n'.format(x_1, x_2, lamda))\n",
    "    solveNewtonBased(x_1, x_2, lamda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "434m87H2zafH"
   },
   "source": [
    "It is obvious that now, using far away points from the optimal solution, the method cannot find the optimal solution. Leading us to the result that the method we already have is not efficint with points that are far away from the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwFhoneaz4Ih"
   },
   "source": [
    "# 3. One way to find the optimal solution from points that are far away of the optimal solution is to start the optimization with another function that allows us to find an approximation to the solution we are looking for. Once an approximate solution is found, we can apply the Newton-based technique we have presented previously to find the optimal solution.\n",
    "\n",
    "# The function that allows us to find an approximation to the solution we are looking for is called, in this context, the merit function. Usually, a merit function is the sum of terms that include the objective function and the amount of infeasibility of the constraints. One example of a merit function for the problem we are treating is the quadratic penalty function  where  is some positive number. The greater the value of ρ, the greater the penalty for infeasibility. The difficulty arises in defining a proper merit function for a particular equality constrained problem.\n",
    "\n",
    "# Here we propose you to take  and perform a classical gradient descent (with backtraking if you want) to find and approximation to the solution we are looking for. Observe if you arrive near to the optimal solution of the problem.Take into account that you may have numerical problems with the gradient. A simple way to deal with it is to normalize the gradient at each iteration, , and use this normalized gradient as search direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQnRLBLK1jRB"
   },
   "source": [
    "Initially we need to define a function that evaluates the merti function and one for the gradient of the merit. Following we define a function for gradient descent and we are using the gradient normalization to avoid numerical problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PhujQSdz2Hr"
   },
   "outputs": [],
   "source": [
    "def merit(x_1, x_2, ro=10):\n",
    "    return f(x_1, x_2) + ro * h(x_1, x_2) ** 2  # Objective + penalty for constraint violation\n",
    "\n",
    "def grad_merit(x_1, x_2, ro=10):\n",
    "    return grad_f(x_1, x_2) + 2 * ro * h(x_1, x_2) * grad_h(x_1, x_2)  # Gradient of f + penalty gradient\n",
    "\n",
    "# Gradient Descent Algorithm: Minimizes the objective function using the gradient descent method\n",
    "# with an adaptive step size based on function evaluations.\n",
    "def gradient_descent(f, grad_f, w0, f_tol=1e-3, grad_tol=1e-5):\n",
    "    x = [w0]  # Store the history of points visited during the optimization\n",
    "\n",
    "    while True:\n",
    "        # Calculate the gradient at the current point\n",
    "        gradient_of_f = grad_f(w0[0], w0[1])\n",
    "\n",
    "        # Normalize the gradient\n",
    "        grad_normalized = gradient_of_f / np.linalg.norm(gradient_of_f)\n",
    "\n",
    "        # Initialize step size to 1\n",
    "        alpha = 1\n",
    "\n",
    "        #  Reduce alpha until the objective function decreases\n",
    "        while f(*(w0 - alpha * grad_normalized)) >= f(*w0):\n",
    "            alpha /= 2  # Halve alpha until the condition is satisfied\n",
    "\n",
    "        # Update the current point\n",
    "        w0 = w0 - alpha * grad_normalized\n",
    "\n",
    "        # Append the updated point to the history\n",
    "        x.append(w0)\n",
    "\n",
    "        #  If the change in function value is smaller than the tolerance, stop.\n",
    "        #  If the gradient's norm is smaller than the gradient tolerance, stop.\n",
    "        gradient_of_f = grad_f(w0[0], w0[1])\n",
    "        grad_normalized = gradient_of_f / np.linalg.norm(gradient_of_f)\n",
    "        if np.abs(f(*x[-1]) - f(*x[-2])) < f_tol or np.linalg.norm(grad_normalized) < grad_tol:\n",
    "            return np.array(x)  # Return the history of points visited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5N3NypP8iGu"
   },
   "source": [
    "We are now using the same 3 random points that are far away from the optimal to test this method using the gradient descent and the merit functions wth normalization for gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16IDpMUc2Fct",
    "outputId": "aa21be7a-0522-47ce-c232-c3eb05cb59f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation: [-0.88379466  0.32295731]\n",
      "Approximation: [-0.82317412  0.54968233]\n",
      "Approximation: [-0.83338108  0.53404357]\n"
     ]
    }
   ],
   "source": [
    "for point in far_away_points:\n",
    "    solution_approx = gradient_descent(merit, grad_merit, point[:2])\n",
    "    print(\"Approximation:\", solution_approx[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jupYxHIx2QVK"
   },
   "source": [
    "Using this method, even with points far away from the optimal (same points we used before) our results are now closer to the optimal solution but not the same. They are like an approximation of the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMBkg-iL6YSO"
   },
   "source": [
    "# 4. As previously commented, the minimizers of the merit function  do not necessarily have to coincide with the minimizers of the constrained problem. Thus, once we “sufficiently” approach the optimal solution we may use the Newton method (with ) to find the solution to the problem.\n",
    "\n",
    "# Therefore the algorithm consists in starting with the Merit function to obtain an approximation to the optimal point we are looking for. Once an approximation to the solution is found,use the Newton-based method to find the optimal solution. Check if you are able to find the optimal solution to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H0V9VNc7m5m"
   },
   "source": [
    "Using the last approximation obtained from merit and gradient descent, we are testing if Newtons method can find the optimal solution from now on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZG0cxRT6a3c",
    "outputId": "af2ddfad-b7de-4310-ec16-5b34acbfe606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point x0 = [-0.833381079215307, 0.5340435724734561]\n",
      "\n",
      "Iterations: 0\n",
      "\n",
      "x = (x_1, x_2) = (-0.80261, 0.60105), lamda = -0.19829\n",
      "\n",
      "Iterations: 1\n",
      "\n",
      "x = (x_1, x_2) = (-0.75282, 0.66301), lamda = -0.20565\n",
      "\n",
      "Iterations: 2\n",
      "\n",
      "x = (x_1, x_2) = (-0.74832, 0.66336), lamda = -0.21228\n"
     ]
    }
   ],
   "source": [
    "solution_x = solveNewtonBased(solution_approx[-1,0], solution_approx[-1,1], lamda=-1, alpha=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUqvgdFD9SRK"
   },
   "source": [
    "We can see that using both methods combined we are now able to find the optimal solution."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
